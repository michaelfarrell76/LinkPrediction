{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pickle\n",
    "import urllib\n",
    "import time\n",
    "import feedparser\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_graph_adj_list(data, authors, adj_list=set(), weighted_adj_list=None):\n",
    "    all_author_pairs = [(min(a1, a2), max(a1, a2)) for (a1, a2) in itertools.combinations(authors, 2)]\n",
    "    all_author_pairs_set = set(all_author_pairs)\n",
    "    if weighted_adj_list is None:\n",
    "        weighted_adj_list = {}\n",
    "        for pair in all_author_pairs:\n",
    "            weighted_adj_list[pair] = 0\n",
    "    \n",
    "    for entry in data:\n",
    "        paper_authors = entry[2]\n",
    "        author_pairs = [(min(a1, a2), max(a1, a2)) for (a1, a2) in itertools.combinations(paper_authors, 2)]\n",
    "        for ap in author_pairs:\n",
    "            if ap[0] in authors and ap[1] in authors:\n",
    "                if ap in all_author_pairs_set:\n",
    "                    adj_list.add(ap)\n",
    "                    weighted_adj_list[ap] += 1\n",
    "                \n",
    "    return adj_list, weighted_adj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adapted from http://export.arxiv.org/api_help/docs/user-manual.html\n",
    "\"\"\"\n",
    "\n",
    "# Base api query url\n",
    "base_url = 'http://export.arxiv.org/api/query?';\n",
    "\n",
    "# Search parameters\n",
    "category = 'astro-ph'\n",
    "search_query = 'cat:' + category # search for electron in all fields\n",
    "start = 0                        # start at the first result\n",
    "total_results = 30000            # want 20 total results\n",
    "results_per_iteration = 2000     # 5 results at a time\n",
    "wait_time = 3                    # number of seconds to wait beetween calls\n",
    "\n",
    "print 'Searching arXiv for %s' % search_query\n",
    "\n",
    "entries = []\n",
    "\n",
    "with open(category + \".txt\", \"w\") as f:\n",
    "\n",
    "    for i in range(start,total_results,results_per_iteration):\n",
    "        print \"Results %i - %i\" % (i,i+results_per_iteration)\n",
    "        query = 'search_query=%s&start=%i&max_results=%i' % (search_query,\n",
    "                                                             i,\n",
    "                                                            results_per_iteration)\n",
    "        # perform a GET request using the base_url and query\n",
    "        response = urllib.urlopen(base_url+query).read()\n",
    "\n",
    "        # parse the response using feedparser\n",
    "        feed = feedparser.parse(response)\n",
    "\n",
    "        # Run through each entry, and print out information\n",
    "        for entry in feed.entries:\n",
    "            title = entry.title.encode('utf-8').replace(\"\\n\", \"\")\n",
    "            year = entry.published_parsed[0]\n",
    "            authors = tuple([str(author['name'].encode('utf-8')) for author in entry.authors])\n",
    "            entries.append((title, year, authors))\n",
    "\n",
    "            f.write(title + \",\" + str(year) + \",\" + \"|\".join(authors) + \"\\n\")\n",
    "\n",
    "        print 'Sleeping for %i seconds' % wait_time \n",
    "        time.sleep(wait_time)\n",
    "\n",
    "# pickle.dump(entries, open(category + '_entries.pkl', 'wb'))\n",
    "# entries = pickle.load(open(category + 'entries.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "authors = set(str.split(\"#\".join([\"#\".join(entry[2]) for entry in entries]), \"#\"))\n",
    "years = set(entry[1] for entry in entries)\n",
    "\n",
    "entries_1998 = filter(lambda entry: entry[1] == 1998, entries)\n",
    "entries_1999 = filter(lambda entry: entry[1] == 1999, entries)\n",
    "entries_2000 = filter(lambda entry: entry[1] == 2000, entries)\n",
    "entries_2001 = filter(lambda entry: entry[1] == 2001, entries)\n",
    "\n",
    "train = entries_1998 + entries_1999\n",
    "test = entries_2000 + entries_2001\n",
    "\n",
    "all_authors_in_train = {author:0 for author in authors}\n",
    "all_authors_in_test = {author:0 for author in authors}\n",
    "\n",
    "datasets = {\"train\": train, \"test\": test}\n",
    "for name, dataset in datasets.iteritems():\n",
    "    for entry in dataset:\n",
    "        paper_authors = entry[2]\n",
    "        for author in paper_authors:\n",
    "            if name == \"train\":\n",
    "                all_authors_in_train[author] += 1 \n",
    "            else:\n",
    "                all_authors_in_test[author] += 1\n",
    "                \n",
    "# Core is subset of authors who have written at least 3 papers during training period\n",
    "# train is 1994 - 1996, test is 1997 - 1999\n",
    "authors_in_train = set([k for k, v in all_authors_in_train.iteritems() if v > 2])\n",
    "authors_in_test = set([k for k, v in all_authors_in_test.iteritems() if v > 2])\n",
    "authors_train_dict = [v for k, v in all_authors_in_train.iteritems() if v > 2]\n",
    "authors_test_dict = [v for k, v in all_authors_in_train.iteritems() if v > 2]\n",
    "authors = sorted(list(authors_in_train.intersection(authors_in_test)))\n",
    "authors = filter(lambda author: len(author) > 3, authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_adj_list_names, weighted_train_adj_list_names = create_graph_adj_list(train, authors)\n",
    "test_adj_list_names, weighted_test_adj_list_names = \\\n",
    "                        create_graph_adj_list(test, authors, adj_list=deepcopy(train_adj_list_names),\n",
    "                        weighted_adj_list = deepcopy(weighted_train_adj_list_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "author_ind = {authors[i]:i for i in range(len(authors))}\n",
    "train_adj_list = map(lambda pair: (author_ind[pair[0]], author_ind[pair[1]]), train_adj_list_names) \n",
    "test_adj_list = map(lambda pair: (author_ind[pair[0]], author_ind[pair[1]]), test_adj_list_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Interesting stats for the graph\n",
    "# Number of nodes: 2338\n",
    "# Number of edges in train: 11801, number of edges in test: 18558\n",
    "# Max number of edges: 2731953\n",
    "# Average weight on each edge without 0s: 3.21\n",
    "# Max weight on each edge is 41\n",
    "# Mean edges per person 5.86\n",
    "# Max edges per node 55 (publications per author)\n",
    "\n",
    "np.mean(filter(lambda x: x != 0, [v for k, v in weighted_test_adj_list_names.iteritems()]))\n",
    "max([v for k, v in weighted_test_adj_list_names.iteritems()])\n",
    "np.mean(authors_test_dict)\n",
    "max(authors_test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(36, 36))\n",
    "train_graph = nx.Graph()\n",
    "for ind, author in author_ind.iteritems():\n",
    "    train_graph.add_node(ind)\n",
    "for e in train_adj_list:\n",
    "    train_graph.add_edge(e[0], e[1])\n",
    "pos = nx.layout.circular_layout(train_graph)\n",
    "edgewidths = 1.0\n",
    "edgecolors = \"blue\"\n",
    "nodesizes = 0.2\n",
    "\n",
    "nx.draw_networkx_edges(train_graph, pos, width=edgewidths,\n",
    "                                edge_color=edgecolors, alpha=0.1)\n",
    "\n",
    "nx.draw_networkx_nodes(train_graph, pos, node_size=nodesizes,\n",
    "                                linewidth=edgewidths,\n",
    "                                node_color=\"black\", alpha=1)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
