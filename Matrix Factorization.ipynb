{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelfarrell/anaconda/lib/python2.7/site-packages/matplotlib/__init__.py:1069: UserWarning: Bad val \"$TEMPLATE_BACKEND\" on line #38\n",
      "\t\"backend      : $TEMPLATE_BACKEND\n",
      "\"\n",
      "\tin file \"/Users/michaelfarrell/.matplotlib/matplotlibrc\"\n",
      "\tKey backend: Unrecognized backend string \"$template_backend\": valid strings are [u'pgf', u'cairo', u'MacOSX', u'CocoaAgg', u'gdk', u'ps', u'GTKAgg', u'nbAgg', u'GTK', u'Qt5Agg', u'template', u'emf', u'GTK3Cairo', u'GTK3Agg', u'WX', u'Qt4Agg', u'TkAgg', u'agg', u'svg', u'GTKCairo', u'WXAgg', u'WebAgg', u'pdf']\n",
      "  (val, error_details, msg))\n",
      "/Users/michaelfarrell/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pickle\n",
    "import urllib\n",
    "import time\n",
    "import feedparser\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import scipy.misc\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Category of papers\n",
    "category = 'astro-ph'\n",
    "\n",
    "# Metadata\n",
    "entries = pickle.load(open(category + '_entries.pkl', 'rb'))\n",
    "author_ind = pickle.load( open(category + '_author_ind.pkl', 'rb'))\n",
    "\n",
    "# List of edges in train/test graphs\n",
    "train_adj_list = pickle.load(open(category + '_train_adj_list.pkl', 'rb'))\n",
    "test_adj_list = pickle.load(open(category + '_test_adj_list.pkl', 'rb'))\n",
    "\n",
    "# List of edges with year added in train/test graphs\n",
    "train_adj_list_w_year = pickle.load( open( category + '_train_adj_list_with_year.pkl'))\n",
    "test_adj_list_w_year = pickle.load( open( category + '_test_adj_list_with_year.pkl'))\n",
    "\n",
    "# Number of authors\n",
    "num_authors = len(author_ind)\n",
    "\n",
    "# List of author ids\n",
    "authors = range(num_authors)\n",
    "\n",
    "# Edges\n",
    "pos_edges = set([(min(a1, a2), max(a1, a2)) for (a1, a2) in \\\n",
    "                 itertools.combinations(authors, 2)]) - set(train_adj_list)\n",
    "pred_edges = set(test_adj_list) - set(train_adj_list)\n",
    "\n",
    "# Years in the training set\n",
    "train_years = sorted(list(set(map(lambda x : x[2], train_adj_list_w_year))))\n",
    "\n",
    "# Years in the testing set\n",
    "test_years = sorted(list(set(map(lambda x : x[2], test_adj_list_w_year))))\n",
    "\n",
    "# All possible edges\n",
    "possible_edges = list(itertools.combinations(authors, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the edges up by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "edges_by_year = {}\n",
    "for year in test_years:\n",
    "    edges_by_year[year] = map(lambda y: y[:2], filter(lambda x : x[2] == year, test_adj_list_w_year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1996, 1358)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(1997, 2250)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(1998, 5716)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(1999, 4751)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2000, 4889)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2001, 2948)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for year in test_years:\n",
    "    year, len(edges_by_year[year])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and validation years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('train years', [1996, 1997, 1998], 'validation years', 1999)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years_for_train = train_years[:3]\n",
    "valid_year = train_years[3]\n",
    "'train years', years_for_train, 'validation years', valid_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the set of edges in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9324"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = set()\n",
    "for year in years_for_train:\n",
    "    train = train.union(set(edges_by_year[year]))\n",
    "n_train = len(train)\n",
    "n_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label all edges as 1 for edge and 0 for no edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targets = map(lambda x : int(x in train), possible_edges)\n",
    "zipped_train_input = zip(possible_edges, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation set of new edges added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4751"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid = set(edges_by_year[valid_year]).difference(train)\n",
    "n_valid = len(valid)\n",
    "n_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The set of labled edges to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "potential_new_edges = set(map(lambda  y : y[0], filter(lambda x: x[1] == 0, zipped_train_input)))\n",
    "zipped_valid_input = map(lambda x : (x, 1) if x in valid else (x,0), potential_new_edges)\n",
    "valid_no_edges = filter(lambda x: x[1] == 0, zipped_valid_input)\n",
    "valid_yes_edges = filter(lambda x: x[1] == 1, zipped_valid_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n",
      "Avg Train Loss: 22.168088737\n",
      "Avg Validation Loss: 22.1763900657\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'valid_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-0c23db844068>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'center left'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_to_anchor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m \u001b[0mprint_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;31m# SGD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-0c23db844068>\u001b[0m in \u001b[0;36mprint_status\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mno_preds\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_no_edges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mno_avgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mno_meds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'valid_preds' is not defined"
     ]
    }
   ],
   "source": [
    "k = 10 # Number of latent features\n",
    "gammaU = 1e-3 # Learning rate for latent feature vectors\n",
    "gammaB = 1e-3 # Learning rate for node biases\n",
    "lambdaU = 1e-1 # regularization parameter for latent feature vecotrs\n",
    "TOP_K = n_valid # K to use for top k accuracy\n",
    "N_EPOCH = 25 # Number of epochs to train\n",
    "link = lambda x : x # the link function mapping score to prediction\n",
    "loss = lambda (p, t): (p - t)**2 # loss function takes in prediction and target and returns loss\n",
    "score = lambda i, j: np.dot(U[i], U[j]) + B[i] + B[j] # Takes in two nodes and scores their probability of an edge\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "U = np.random.rand(num_authors,k) # latent feature matrix\n",
    "B = np.random.rand(num_authors) # Biases\n",
    "\n",
    "# Holding the train/validation losses\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "# Holds the average/medians prediction score for edges being added and not added\n",
    "no_avgs = []\n",
    "no_meds = []\n",
    "yes_avgs = []\n",
    "yes_meds = []\n",
    "\n",
    "# Differences between yes_avgs and no_avgs\n",
    "diff_avgs = []\n",
    "\n",
    "# Accuracy on TOP_K\n",
    "topk_acc = []\n",
    "\n",
    "\n",
    "def predict(vals):\n",
    "    '''\n",
    "        Give the zipped input, returned the prediction and target value in a \n",
    "        list of tuples\n",
    "        \n",
    "        (pred, targ)\n",
    "    '''\n",
    "    return map(lambda ((i,j), t) : (link(score(i, j)), t), vals)\n",
    "\n",
    "def calc_loss(vals):\n",
    "    '''\n",
    "        Return the loss for the current state \n",
    "    '''\n",
    "    # Regularizer\n",
    "    reg_term = lambdaU * np.linalg.norm(U, ord='fro')\n",
    "    \n",
    "    # Make predictions\n",
    "    preds = predict(vals)\n",
    "    \n",
    "    # Calculate loss with regularization\n",
    "    return np.mean(map(loss, preds)) + reg_term\n",
    "\n",
    "def top_acc(vals):\n",
    "    '''\n",
    "        Determine our TOP_K rated edges and see if they are in the set\n",
    "        of edges that were added in the validations et\n",
    "    '''\n",
    "    predictions = predict(vals)\n",
    "    \n",
    "    # Sort predictions \n",
    "    sorted_scores = sorted(predictions, key= lambda  x: x[0], reverse=True)\n",
    "    \n",
    "    # Sett how many of the top predictions have an edge\n",
    "    return len(filter(lambda x : x[1] == 1, sorted_scores[:TOP_K])) / float(TOP_K)\n",
    "\n",
    "def print_status(epoch):\n",
    "    '''\n",
    "        Prints the current status of the SGD operation\n",
    "    '''\n",
    "    print 'EPOCH', epoch\n",
    "\n",
    "    train_losses.append(calc_loss(zipped_train_input))\n",
    "    print 'Avg Train Loss:',train_losses[epoch]\n",
    "\n",
    "    valid_losses.append(calc_loss(zipped_valid_input))\n",
    "    print 'Avg Validation Loss:', valid_losses[epoch]\n",
    "    \n",
    "    no_preds =  map(lambda x : x[0], predict(valid_no_edges))\n",
    "    no_avgs.append(np.mean(valid_preds))\n",
    "    no_meds.append(np.median(valid_preds))\n",
    "    \n",
    "    yes_preds = map(lambda x : x[0],predict(valid_yes_edges))\n",
    "    yes_avgs.append(np.mean(yes_preds))\n",
    "    yes_meds.append(np.median(yes_preds))\n",
    "    print 'Avg Score for missing edges:', no_avgs[epoch], 'Avg Score for added edges:', yes_avgs[epoch]\n",
    "    print 'Median Score for missing edges:', no_meds[epoch], 'Media Score for added edges:', yes_meds[epoch]\n",
    "\n",
    "    diff_avgs.append(yes_avgs[epoch] - no_avgs[epoch])\n",
    "    print 'Avg Difference:', diff_avgs[epoch]\n",
    "\n",
    "    topk_acc.append(top_acc(zipped_valid_input))\n",
    "    print 'Accuracy on Top', TOP_K, ':', topk_acc[epoch]\n",
    "\n",
    "def plot_results(epoch):\n",
    "    '''\n",
    "        Plots the results vs the epochs\n",
    "    '''\n",
    "    fig, ax = plt.subplots(2,1, figsize = (6,12))\n",
    "    \n",
    "    # PLot score differential\n",
    "    _ = ax[0].plot(range(epoch + 1), diff_avgs, label = 'Difference in Avg Score between edge and no edge')\n",
    "    _ = ax[0].set_title('Performance Over Epochs')\n",
    "    _ = ax[0].set_xlabel('Epoch')\n",
    "    _ = ax[0].set_ylabel('Score Difference')\n",
    "    \n",
    "    # On same plot plot accuracy on top k\n",
    "    ax2 = ax[0].twinx()\n",
    "    _ = ax2.plot(range(epoch + 1), topk_acc, 'r', label = 'Accuracy on Top %s' % TOP_K)\n",
    "    _ = ax2.set_ylabel('Accuracy')\n",
    "    _ = ax2.legend(loc='center left', bbox_to_anchor=(1.25, 0.1))\n",
    "    _ = ax[0].legend(loc='center left', bbox_to_anchor=(1.25, 0.5))\n",
    "    \n",
    "    # On second plot plot loss\n",
    "    _ = ax[1].plot(range(epoch + 1), valid_losses, label = 'Avg Validation Loss')\n",
    "    _ = ax[1].plot(range(epoch + 1), train_losses, label = 'Avg Training Loss')\n",
    "    _ = ax[1].set_xlabel('Epoch')\n",
    "    _ = ax[1].set_ylabel('Loss')\n",
    "    _ = ax[1].legend(loc='center left', bbox_to_anchor=(1.25, 0.5))\n",
    "    \n",
    "print_status(0)\n",
    "\n",
    "# SGD\n",
    "for epoch in range(1, N_EPOCH+1):\n",
    "\n",
    "    # Shuffle the training values\n",
    "    np.random.shuffle(zipped_train_input)\n",
    "    \n",
    "    # Iterate through possible edges and update parameters\n",
    "    for (i,j), target in tqdm(zipped_train_input):\n",
    "        eij = score(i,j) - target\n",
    "        U[i] -= gammaU*(eij*U[j] + lambdaU*U[i])\n",
    "        U[j] -= gammaU*(eij*U[i] + lambdaU*U[j])\n",
    "        B[i] -= gammaU*eij\n",
    "        B[j] -= gammaU*eij\n",
    "    \n",
    "    # Clear the display\n",
    "    display.clear_output(wait=True)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Print out the results of the epoch\n",
    "    print_status(epoch)\n",
    "    \n",
    "    # Plot and saves results\n",
    "    plot_results(epoch)\n",
    "    \n",
    "    # Display plot\n",
    "    display.display(plt.gcf())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
